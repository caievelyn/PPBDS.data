## Overview

We are creating an R package, PPBDS.data, which will contain a collection of interesting tibbles, with a focus on political science and designed for classroom use. Their main use will be for Gov 50 and for Preceptor's Primer. Each tibble will have a small number of variables, about as many as will fit across the screen when the tibble is printed. (There may be exceptions to this, but probably not. Tibbles need to be simple since we don't want to overwhelm readers. The focus should be on the code and the models, not on the data.)

### Instructions

* Write scripts which create our data sets and adds them to our PPBDS.data package. Write associated help files. Skim [*R Packages*](https://r-pkgs.org/), but read closely the [data section](https://r-pkgs.org/data.html). Examine [covdata package](https://kjhealy.github.io/covdata/). Use **usethis** and **pkgdown** packages.

* Follow the conventions with the train data: make_trains.R (in data-raw/), trains.rda (in data/), trains.R (in R/). Note how the name of the tibble appears in the file name of each of the three key things? Main workflow is that, unless too big, raw data is placed in the data-raw/ directory. It is read and cleaned in the make_name-of-tibble.R script, the last step of which is to create the name-of-tibble.rda file, using `usethis::use_data(name-of-tibble, overwrite = TRUE)`. The make_name-of-tibble.R has lots and lots of documentation, including the details of where the original data came from. (At some point, we might turn this into a function, as Healy does, but not right now.) name-of-tibble.R has the information for R to create the help file. You can save this until we decide on the variables that we want.

### pkgdown

* We are currently using pkgdown to make our website. But it is confusing and a bit fragile. Nor have I done a good job of documenting my lessons. You need to use build_site() to create the site after you have made some changes.

* Even though I only did this a few weeks ago, I have already forgotten the key steps. Weak! Document better!

### git-lfs

Note that you need to have git-lfs installed to work with our repo because we use some big data sets. See [here](https://git-lfs.github.com/) for instructions. Note that the current Mac OS (Catalina) will give you a bit of a problem. To solve it, I did:

brew install git-lfs
xattr -d com.apple.quarantine /usr/local/bin/git-lfs

The first command is just a standard way of installing things on the Mac. The second step was necessary because Apple has tighted the safety checks on downloaded programs. At some point, only the first step will be necessary. But, for now, you need to do the second to confirm to Apple that git-lfs is safe.

Once you have done this, all your standard git commands will work as usual. However, do we really want to distribute such huge files with the package? Probably not. Users don't want all that junk. They just want the cleaned up data. Ahh! Which is why data-raw/ in in .Rbuildignore, so that directory is not included in the distributed package. Cool!


### Data Sets

### Debi

* Called `sps`. Want some continuous variables. Something from [this experiment](https://gking.harvard.edu/category/research-interests/applications/mexican-health-care-evaluation) in Mexico. Connect closely to the Lancet paper.

### Beau

* Called `nes`. [National Election Survey](https://electionstudies.org/) data, similar to what we have been working with in 1006.

### Andy

* Called `qscores` New Q-Guide data. Original format is as a database, so some wizardry is required. It --- info.db --- has been placed in data-raw/. Looks like the **RSQLite** package is the best approach. Credit Aurash Vatan '23 for providing the data in the help page. From him: The important table is called "classes" with the following format:

id | name | enrollment | overall | workload | department | term | course_number

99 | AFRAMER 100Y: Introduction to Black Poetry | 49 | 4.2 | 2.6 | AFRAMER | 2019S | 100Y

Three other potentially useful tables: "reviews" has the text of all the reviews in "classes", and "people" and "professors" together store the names of professors and which classes they taught. Some of the names that I have used for columns may be slightly ambiguous, but hopefully that contributes some useful messiness to the data! There are a few more tables in the file, but they were purely for the functioning of the website and so are irrelevant.

### Ishan

Exploratory work with cces (Preceptor told me to try this one out).

### TBA

Don't spend more than a few hours on any of these without stopping and then having a conversation with me about them. Not all of them can work! Don't pound your head against the wall.



* Called `cces`. See how we use it in the *Primer* chapter 14. Mention Shiro's work [here](https://github.com/kuriwaki/cces_cumulative) in the help page. You can just get the cleaned up data form here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/II2DB6. Download it by hand. (I did this already, but you should document the details. I am not sure if we can automate it, or at least make it a part of the script.) We will not keep all the variables. But which ones should we keep? And should we clean up the values for category variables, for example? Maybe we would be better off getting the dta and then using the **haven** to magically set the levels correctly, which is something which Stata makes easier for factor variables. Read the associated PDF. Perhaps the variables should be simple characters? Only other reasonable choice is factors. It is stupid to use "1" to mean "Democrat."

* Called `congress`. Something about the results of US congressional elections over several election cycles. 538? Really want this to include some measure of campaign spending so we can discuss a (potential) causal effect. Didn't they have a model which they used for the 2018 election? If they estimated a model, they must have some data.

* Called `shaming`, if we use this https://isps.yale.edu/sites/default/files/publication/2012/12/ISPS08-001.pdf and data is in the Dataverse. Useful discussion in chapter 2 of QSS.

* Or maybe a better voting experiment would be Electoral Administration in Fledgling Democracies: Experimental Evidence from Kenya at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/UT25HQ. Kenya better because it is international, and we can use Erin's code!

* [Open police data](https://openpolicing.stanford.edu/data/). Something with millions of rows and, ideally, geographic coordinates.

* Need something from Matt Blackwell's work on slavery. Ultimate use may just be in making an example for the maps appendix, which should be longer.

* Maybe [GSS](http://gss.norc.org/). Isn't there already an R package?

* FEC data. This is complex and important enough that it might become a stand-alone package. See [DIME](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/O5PX0B&version=2.2).

* Shiro: I also have a clean large dataset of satellite lights in Africa from this period using this paper: https://www.aeaweb.org/articles?id=10.1257/aer.20161385

* Call `deaths`. Maybe that demography data we used on the last day of 1005 this semester? Or is that too big?


